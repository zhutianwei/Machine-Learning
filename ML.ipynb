{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"name":"python","version":"3.7.1","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"toc_position":{},"skip_h1_title":false,"number_sections":true,"title_cell":"Table of Contents","toc_window_display":false,"base_numbering":1,"toc_section_display":true,"title_sidebar":"Contents","toc_cell":true,"nav_menu":{"height":"299px","width":"351px"},"sideBar":false},"kernel_info":{"name":"python3"},"nteract":{"version":"0.15.0"},"colab":{"name":"ML.ipynb","provenance":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"toc":true,"id":"U911rHwtG0it","colab_type":"text"},"source":["<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n","<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Intro-to-Machine-Learning\" data-toc-modified-id=\"Intro-to-Machine-Learning-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Intro to Machine Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-the-data\" data-toc-modified-id=\"Get-the-data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Get the data</a></span></li><li><span><a href=\"#Visualization.-Data-Science\" data-toc-modified-id=\"Visualization.-Data-Science-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Visualization. Data Science</a></span></li><li><span><a href=\"#Prepare-the-data-for-Machine-Learning-algorithms\" data-toc-modified-id=\"Prepare-the-data-for-Machine-Learning-algorithms-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Prepare the data for Machine Learning algorithms</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Cleaning\" data-toc-modified-id=\"Data-Cleaning-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Data Cleaning</a></span></li><li><span><a href=\"#Handling-Text-and-Categorical-Attributes\" data-toc-modified-id=\"Handling-Text-and-Categorical-Attributes-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Handling Text and Categorical Attributes</a></span></li><li><span><a href=\"#Custom-Transformers\" data-toc-modified-id=\"Custom-Transformers-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Custom Transformers</a></span></li><li><span><a href=\"#Feature-Scaling\" data-toc-modified-id=\"Feature-Scaling-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>Feature Scaling</a></span></li><li><span><a href=\"#Transformation-Pipelines\" data-toc-modified-id=\"Transformation-Pipelines-1.3.5\"><span class=\"toc-item-num\">1.3.5&nbsp;&nbsp;</span>Transformation Pipelines</a></span></li></ul></li><li><span><a href=\"#Select-and-train-a-model\" data-toc-modified-id=\"Select-and-train-a-model-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Select and train a model</a></span></li><li><span><a href=\"#Fine-tune-your-model\" data-toc-modified-id=\"Fine-tune-your-model-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Fine-tune your model</a></span></li></ul></li><li><span><a href=\"#Classification\" data-toc-modified-id=\"Classification-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Performance-Measures\" data-toc-modified-id=\"Performance-Measures-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Performance Measures</a></span><ul class=\"toc-item\"><li><span><a href=\"#Error-Analysis\" data-toc-modified-id=\"Error-Analysis-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Error Analysis</a></span></li></ul></li></ul></li><li><span><a href=\"#Training-Models\" data-toc-modified-id=\"Training-Models-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Training Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Linear-Regression\" data-toc-modified-id=\"Linear-Regression-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Linear Regression</a></span></li><li><span><a href=\"#Polynomial-Regression\" data-toc-modified-id=\"Polynomial-Regression-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Polynomial Regression</a></span></li><li><span><a href=\"#Learning-Curves\" data-toc-modified-id=\"Learning-Curves-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Learning Curves</a></span></li><li><span><a href=\"#Regularized-Linear-Models\" data-toc-modified-id=\"Regularized-Linear-Models-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Regularized Linear Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Ridge-Regression\" data-toc-modified-id=\"Ridge-Regression-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Ridge Regression</a></span></li><li><span><a href=\"#Lasso-Regression\" data-toc-modified-id=\"Lasso-Regression-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>Lasso Regression</a></span></li><li><span><a href=\"#Elastic-Net\" data-toc-modified-id=\"Elastic-Net-3.4.3\"><span class=\"toc-item-num\">3.4.3&nbsp;&nbsp;</span>Elastic Net</a></span></li><li><span><a href=\"#Early-Stopping\" data-toc-modified-id=\"Early-Stopping-3.4.4\"><span class=\"toc-item-num\">3.4.4&nbsp;&nbsp;</span>Early Stopping</a></span></li></ul></li><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Logistic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Softmax-Regression\" data-toc-modified-id=\"Softmax-Regression-3.5.1\"><span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>Softmax Regression</a></span></li></ul></li></ul></li><li><span><a href=\"#SVM\" data-toc-modified-id=\"SVM-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>SVM</a></span><ul class=\"toc-item\"><li><span><a href=\"#Linear-SVM-Classification\" data-toc-modified-id=\"Linear-SVM-Classification-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Linear SVM Classification</a></span></li><li><span><a href=\"#Nonlinear-SVM-Classification\" data-toc-modified-id=\"Nonlinear-SVM-Classification-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Nonlinear SVM Classification</a></span></li><li><span><a href=\"#SVM-Regression\" data-toc-modified-id=\"SVM-Regression-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>SVM Regression</a></span></li></ul></li><li><span><a href=\"#Decision-Tree\" data-toc-modified-id=\"Decision-Tree-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Decision Tree</a></span></li><li><span><a href=\"#Ensemble-Learning\" data-toc-modified-id=\"Ensemble-Learning-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Ensemble Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bagging-and-Pasting\" data-toc-modified-id=\"Bagging-and-Pasting-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Bagging and Pasting</a></span></li><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Random Forest</a></span></li></ul></li><li><span><a href=\"#Neural-Networks\" data-toc-modified-id=\"Neural-Networks-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Neural Networks</a></span></li><li><span><a href=\"#TensorFlow\" data-toc-modified-id=\"TensorFlow-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>TensorFlow</a></span></li><li><span><a href=\"#Keras\" data-toc-modified-id=\"Keras-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Keras</a></span></li></ul></div>"]},{"cell_type":"code","metadata":{"id":"gtUln7DyG0it","colab_type":"code","colab":{}},"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SPfEUxAtG0ix","colab_type":"code","colab":{}},"source":["%conda install seaborn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d4BMBy4uG0iz","colab_type":"text"},"source":["# Intro to Machine Learning"]},{"cell_type":"markdown","metadata":{"id":"uxi5Hh5VG0i0","colab_type":"text"},"source":["## Get the data"]},{"cell_type":"code","metadata":{"id":"b6YhUbEQG0i0","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","\n","train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IeY8m24oG0i2","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import StratifiedShuffleSplit\n","\n","split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dope2KxUG0i5","colab_type":"text"},"source":["## Visualization. Data Science"]},{"cell_type":"markdown","metadata":{"id":"i2YDLzaRG0i5","colab_type":"text"},"source":["## Prepare the data for Machine Learning algorithms"]},{"cell_type":"markdown","metadata":{"id":"srusr_FSG0i5","colab_type":"text"},"source":["### Data Cleaning"]},{"cell_type":"code","metadata":{"id":"ZY0AN4LUG0i6","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import SimpleImputer\n","\n","imputer = SimpleImputer(strategy=\"median\")\n","imputer.fit(housing_num) # Calculate the median of each column\n","X = imputer.transform(housing_num) \n","\n","# X = imputer.fit_transform(housing_num)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mq7pwybEG0i8","colab_type":"text"},"source":["### Handling Text and Categorical Attributes"]},{"cell_type":"code","metadata":{"id":"_KT5JTy-G0i9","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import OrdinalEncoder\n","\n","ordinal_encoder = OrdinalEncoder()\n","housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cLXs8sxyG0i_","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import OneHotEncoder\n","\n","cat_encoder = OneHotEncoder()\n","housing_cat_1hot = cat_encoder.fit_transform(housing_cat) # sparse matrix\n","housing_cat_1hot.toarray() # array "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SdB0R_5zG0jB","colab_type":"text"},"source":["### Custom Transformers"]},{"cell_type":"code","metadata":{"id":"4OyJGghnG0jC","colab_type":"code","colab":{}},"source":["from sklearn.base import BaseEstimator, TransformerMixin"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mlX8p7UGG0jE","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import FunctionTransformer\n","\n","attr_adder = FunctionTransformer(add_extra_features, validate=False,\n","                                 kw_args={\"add_bedrooms_per_room\": False})\n","housing_extra_attribs = attr_adder.fit_transform(housing.values)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YOnAMMj-G0jG","colab_type":"text"},"source":["### Feature Scaling"]},{"cell_type":"code","metadata":{"id":"TZqub18AG0jG","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import StandardScaler"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9vClsRAbG0jJ","colab_type":"text"},"source":["### Transformation Pipelines"]},{"cell_type":"code","metadata":{"id":"1utMjSPDG0jJ","colab_type":"code","colab":{}},"source":["from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","num_pipeline = Pipeline([\n","        ('imputer', SimpleImputer(strategy=\"median\")),\n","        ('attribs_adder', FunctionTransformer(add_extra_features, validate=False)),\n","        ('std_scaler', StandardScaler()),\n","    ])\n","\n","housing_num_tr = num_pipeline.fit_transform(housing_num)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yol4jPDjG0jM","colab_type":"code","colab":{}},"source":["# earlier versions of the book applied different transformations to different columns using a solution \n","# based on a DataFrameSelector transformer and a FeatureUnion (see below). \n","# It is now preferable to use the ColumnTransformer class\n","from sklearn.compose import ColumnTransformer\n","\n","num_attribs = list(housing_num)\n","cat_attribs = [\"ocean_proximity\"]\n","\n","full_pipeline = ColumnTransformer([\n","        (\"num\", num_pipeline, num_attribs),\n","        (\"cat\", OneHotEncoder(), cat_attribs),\n","    ])\n","\n","housing_prepared = full_pipeline.fit_transform(housing)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oRkgdevxG0jO","colab_type":"code","colab":{}},"source":["from sklearn.pipeline import FeatureUnion\n","\n","num_attribs = list(housing_num)\n","cat_attribs = [\"ocean_proximity\"]\n","\n","old_num_pipeline = Pipeline([\n","        ('selector', OldDataFrameSelector(num_attribs)),\n","        ('imputer', SimpleImputer(strategy=\"median\")),\n","        ('attribs_adder', FunctionTransformer(add_extra_features, validate=False)),\n","        ('std_scaler', StandardScaler()),\n","    ])\n","\n","old_cat_pipeline = Pipeline([\n","        ('selector', OldDataFrameSelector(cat_attribs)),\n","        ('cat_encoder', OneHotEncoder(sparse=False)),\n","    ])\n","\n","old_full_pipeline = FeatureUnion(transformer_list=[\n","        (\"num_pipeline\", old_num_pipeline),\n","        (\"cat_pipeline\", old_cat_pipeline),\n","    ])\n","\n","old_housing_prepared = old_full_pipeline.fit_transform(housing) # The result is the same as with the ColumnTransformer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w9r1oqCbG0jR","colab_type":"text"},"source":["## Select and train a model"]},{"cell_type":"code","metadata":{"id":"P8k2ezKPG0jR","colab_type":"code","colab":{}},"source":["from sklearn.learning_curve import validation_curve\n","degree = np.arange(0, 21) \n","train_score, val_score = validation_curve(PolynomialRegression(), X, y, 'polynomialfeatures__degree', degree, cv=7)\n","plt.plot(degree, np.median(train_score, 1), color='blue', label='training score')\n","plt.plot(degree, np.median(val_score, 1), color='red', label='validation score')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IKgzI2TzG0jT","colab_type":"code","colab":{}},"source":["from sklearn.learning_curve import learning_curve"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ebRs3CwXG0jV","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import LinearRegression\n","\n","lin_reg = LinearRegression()\n","lin_reg.fit(housing_prepared, housing_labels)\n","lin_reg.predict(some_data_prepared)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vnFuNwPiG0jY","colab_type":"code","colab":{}},"source":["from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_error\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pkeftmDCG0ja","colab_type":"code","colab":{}},"source":["from sklearn.tree import DecisionTreeRegressor\n","\n","tree_reg = DecisionTreeRegressor(random_state=42)\n","tree_reg.fit(housing_prepared, housing_labels)\n","housing_predictions = tree_reg.predict(housing_prepared)\n","\n","forest_mse = mean_squared_error(housing_labels, housing_predictions)\n","forest_rmse = np.sqrt(forest_mse)\n","\n","accuracy_score(housing_labels, housing_predictions)\n","x = confusion_matrix(housing_labels, housing_predictions)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6LKzYBpzG0jd","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import cross_val_score\n","\n","scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n","                         scoring=\"neg_mean_squared_error\", cv=10)\n","tree_rmse_scores = np.sqrt(-scores)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qyr8r2GIG0jf","colab_type":"code","colab":{}},"source":["from sklearn.ensemble import RandomForestRegressor\n","\n","forest_reg = RandomForestRegressor(n_estimators=10, random_state=42)\n","forest_reg.fit(housing_prepared, housing_labels)\n","housing_predictions = forest_reg.predict(housing_prepared)\n","\n","forest_mse = mean_squared_error(housing_labels, housing_predictions)\n","forest_rmse = np.sqrt(forest_mse)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WKhMxx1MG0ji","colab_type":"code","colab":{}},"source":["from sklearn.svm import SVR\n","\n","svm_reg = SVR(kernel=\"linear\")\n","svm_reg.fit(housing_prepared, housing_labels)\n","housing_predictions = svm_reg.predict(housing_prepared)\n","\n","svm_mse = mean_squared_error(housing_labels, housing_predictions)\n","svm_rmse = np.sqrt(svm_mse)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cmJCnu7IG0jk","colab_type":"text"},"source":["## Fine-tune your model"]},{"cell_type":"code","metadata":{"id":"_0CF9j2eG0jk","colab_type":"code","colab":{}},"source":["# Here is an example of using grid search to find the optimal polynomial model.\n","from sklearn.model_selection import GridSearchCV\n","param_grid = {'polynomialfeatures__degree': np.arange(21),\n","              'linearregression__fit_intercept': [True, False], \n","              'linearregression__normalize': [True, False]}\n","\n","grid = GridSearchCV(PolynomialRegression(), param_grid, cv=7)\n","grid.fit(X, y)\n","grid.best_params_\n","model = grid.best_estimator_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hkUZ1BodG0jn","colab_type":"code","colab":{}},"source":["# The following code searches for the best combination of hyperparameter values for the RandomForestRegressor\n","from sklearn.model_selection import GridSearchCV\n","\n","param_grid = [\n","    # try 12 (3×4) combinations of hyperparameters\n","    {'n_estimators': [3, 10, 30], \n","     'max_features': [2, 4, 6, 8]},\n","    # then try 6 (2×3) combinations with bootstrap set as False\n","    {'bootstrap': [False], \n","     'n_estimators': [3, 10], \n","     'max_features': [2, 3, 4]},\n","  ]\n","\n","forest_reg = RandomForestRegressor(random_state=42)\n","# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n","grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n","                           scoring='neg_mean_squared_error', return_train_score=True)\n","grid_search.fit(housing_prepared, housing_labels)\n","grid_search.best_params_\n","model = grid_search.best_estimator_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C2FO4cZgG0jp","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import RandomizedSearchCV\n","from scipy.stats import randint\n","\n","param_distribs = {\n","        'n_estimators': randint(low=1, high=200),\n","        'max_features': randint(low=1, high=8),\n","    }\n","\n","forest_reg = RandomForestRegressor(random_state=42)\n","rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n","                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\n","rnd_search.fit(housing_prepared, housing_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Z4QnUnjG0js","colab_type":"code","colab":{}},"source":["# Anaylize the Best Models and Their Errors¶\n","feature_importances = grid_search.best_estimator_.feature_importances_\n","\n","extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n","#cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"] # old solution\n","cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n","cat_one_hot_attribs = list(cat_encoder.categories_[0])\n","attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n","sorted(zip(feature_importances, attributes), reverse=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pc9uXuptG0jt","colab_type":"code","colab":{}},"source":["# Evaluate on the Test Set\n","final_model = grid_search.best_estimator_\n","print(type(final_model))\n","X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n","y_test = strat_test_set[\"median_house_value\"].copy()\n","\n","X_test_prepared = full_pipeline.transform(X_test)\n","final_predictions = final_model.predict(X_test_prepared)\n","\n","final_mse = mean_squared_error(y_test, final_predictions)\n","final_rmse = np.sqrt(final_mse)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t6dXP4rWG0jv","colab_type":"text"},"source":["# Classification"]},{"cell_type":"markdown","metadata":{"id":"XJ_geynKG0jw","colab_type":"text"},"source":["## Performance Measures"]},{"cell_type":"code","metadata":{"id":"tCblZpuEG0jw","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import cross_val_predict\n","\n","y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv = 3)\n","y_train_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LiIZgRAlG0jy","colab_type":"code","colab":{}},"source":["from sklearn.metrics import precision_score, recall_score\n","\n","precision_score(y_train_5, y_train_pred)\n","recall_score(y_train_5, y_train_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nq4WnFuEG0j1","colab_type":"code","colab":{}},"source":["from sklearn.metrics import f1_score\n","f1_score(y_train_5, y_train_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vQ002w-RG0j3","colab_type":"code","colab":{}},"source":["y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n","                             method=\"decision_function\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cn4ZrCaKG0j4","colab_type":"text"},"source":["### Error Analysis"]},{"cell_type":"code","metadata":{"id":"Noyquz5MG0j5","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"edBP6EfPG0j7","colab_type":"text"},"source":["# Training Models"]},{"cell_type":"markdown","metadata":{"id":"7Y7cEerNG0j8","colab_type":"text"},"source":["## Linear Regression"]},{"cell_type":"code","metadata":{"id":"RngGpHogG0j8","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import LinearRegression\n","lin_reg = LinearRegression()\n","lin_reg.fit(X, y)\n","lin_reg.predict(X_new)\n","lin_reg.intercept_, lin_reg.coef_"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1WxgFVyIG0j-","colab_type":"text"},"source":["## Polynomial Regression"]},{"cell_type":"code","metadata":{"id":"czSg7_kdG0j_","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import PolynomialFeatures\n","poly_features = PolynomialFeatures(degree=2, include_bias=False)\n","X_poly = poly_features.fit_transform(X)\n","\n","lin_reg = LinearRegression()\n","lin_reg.fit(X_poly, y)\n","lin_reg.predict(X_poly_new)\n","lin_reg.intercept_, lin_reg.coef_"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BMs5jbIkG0kA","colab_type":"text"},"source":["## Learning Curves"]},{"cell_type":"code","metadata":{"id":"_rjQdWYZG0kB","colab_type":"code","colab":{}},"source":["from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import train_test_split\n","\n","def plot_learning_curves(model, X, y):\n","    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)\n","    train_errors, val_errors = [], []\n","    \n","    for m in range(1, len(X_train)):\n","        model.fit(X_train[:m], y_train[:m])\n","        y_train_predict = model.predict(X_train[:m])\n","        y_val_predict = model.predict(X_val)\n","        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n","        val_errors.append(mean_squared_error(y_val, y_val_predict))\n","\n","    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n","    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n","    plt.legend(loc=\"upper right\", fontsize=14)   # not shown in the book\n","    plt.xlabel(\"Training set size\", fontsize=14) # not shown\n","    plt.ylabel(\"RMSE\", fontsize=14)              # not shown"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IfG2cgYfG0kD","colab_type":"code","colab":{}},"source":["lin_reg = LinearRegression()\n","plot_learning_curves(lin_reg, X, y)\n","plt.axis([0, 80, 0, 3])                         # not shown in the book\n","save_fig(\"underfitting_learning_curves_plot\")   # not shown\n","plt.show()                                      # not shown"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SqX61xU7G0kF","colab_type":"code","colab":{}},"source":["from sklearn.pipeline import Pipeline\n","\n","polynomial_regression = Pipeline([\n","        (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n","        (\"lin_reg\", LinearRegression()),\n","    ])\n","\n","plot_learning_curves(polynomial_regression, X, y)\n","plt.axis([0, 80, 0, 3])           # not shown\n","save_fig(\"learning_curves_plot\")  # not shown\n","plt.show()                        # not shown"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F2Q-rGt2G0kK","colab_type":"text"},"source":["## Regularized Linear Models"]},{"cell_type":"markdown","metadata":{"id":"OtyjgIc6G0kK","colab_type":"text"},"source":["### Ridge Regression"]},{"cell_type":"code","metadata":{"id":"glVih1R1G0kL","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import Ridge\n","ridge_reg = Ridge(alpha=1, solver=\"cholesky\", random_state=42)\n","ridge_reg.fit(X, y)\n","ridge_reg.predict([[1.5]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5yAgW2O7G0kO","colab_type":"code","colab":{}},"source":["# Stochastic Gradient Descent\n","sgd_reg = SGDRegressor(max_iter=50, tol=-np.infty, penalty=\"l2\", random_state=42)\n","sgd_reg.fit(X, y.ravel())\n","sgd_reg.predict([[1.5]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zTebmgBQG0kQ","colab_type":"code","colab":{}},"source":["ridge_reg = Ridge(alpha=1, solver=\"sag\", random_state=42)\n","ridge_reg.fit(X, y)\n","ridge_reg.predict([[1.5]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xUQ60D8GG0kT","colab_type":"text"},"source":["### Lasso Regression"]},{"cell_type":"code","metadata":{"id":"d12oP_9bG0kT","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import Lasso\n","lasso_reg = Lasso(alpha=0.1)\n","lasso_reg.fit(X, y)\n","lasso_reg.predict([[1.5]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DZPkjATEG0kV","colab_type":"text"},"source":["### Elastic Net"]},{"cell_type":"code","metadata":{"id":"zYUZZpquG0kV","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import ElasticNet\n","elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n","elastic_net.fit(X, y)\n","elastic_net.predict([[1.5]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"It5plvTPG0kZ","colab_type":"text"},"source":["### Early Stopping"]},{"cell_type":"code","metadata":{"id":"oGG_Ul-QG0kZ","colab_type":"code","colab":{}},"source":["from sklearn.base import clone\n","sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True, penalty=None,\n","                       learning_rate=\"constant\", eta0=0.0005, random_state=42)\n","\n","minimum_val_error = float(\"inf\")\n","best_epoch = None\n","best_model = None\n","\n","for epoch in range(1000):\n","    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off\n","    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n","    val_error = mean_squared_error(y_val, y_val_predict)\n","    \n","    if val_error < minimum_val_error:\n","        minimum_val_error = val_error\n","        best_epoch = epoch\n","        best_model = clone(sgd_reg)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"34nZZvbbG0kb","colab_type":"code","colab":{}},"source":["best_epoch, best_model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wxGx7J54G0kd","colab_type":"text"},"source":["## Logistic Regression"]},{"cell_type":"markdown","metadata":{"id":"05YwCqlZG0kd","colab_type":"text"},"source":["### Softmax Regression"]},{"cell_type":"markdown","metadata":{"id":"ryKdcCKvG0ke","colab_type":"text"},"source":["# SVM"]},{"cell_type":"markdown","metadata":{"id":"_12UsAHrG0ke","colab_type":"text"},"source":["## Linear SVM Classification "]},{"cell_type":"code","metadata":{"id":"tcW3DaeiG0kf","colab_type":"code","colab":{}},"source":["from sklearn.svm import SVC, LinearSVC\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","\n","# Not recommended\n","svm_clf = SVC(kernel=\"linear\", C=float(\"inf\"))\n","svm_clf.fit(X, y)\n","\n","# SVM Classifier model\n","svm_clf = Pipeline([\n","        (\"scaler\", StandardScaler()),\n","        (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\", random_state=42)),\n","    ])\n","\n","svm_clf.fit(X, y)\n","svm_clf.predict([[5.5, 1.7]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2wIQWlgJG0kh","colab_type":"text"},"source":["## Nonlinear SVM Classification"]},{"cell_type":"code","metadata":{"id":"bcyFIGGbG0ki","colab_type":"code","colab":{}},"source":["from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","\n","\n","polynomial_svm_clf = Pipeline([\n","        (\"poly_features\", PolynomialFeatures(degree=3)),\n","        (\"scaler\", StandardScaler()),\n","        (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\", random_state=42))\n","    ])\n","\n","polynomial_svm_clf.fit(X, y)\n","\n","# Polynomial Kernel\n","poly_kernel_svm_clf = Pipeline([\n","        (\"scaler\", StandardScaler()),\n","        (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n","    ])\n","poly_kernel_svm_clf.fit(X, y)\n","\n","# Gaussian RBF Kernel\n","rbf_kernel_svm_clf = Pipeline([\n","        (\"scaler\", StandardScaler()),\n","        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n","    ])\n","rbf_kernel_svm_clf.fit(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GBCIoOOzG0kk","colab_type":"text"},"source":["## SVM Regression"]},{"cell_type":"code","metadata":{"id":"gMfEigSJG0kl","colab_type":"code","colab":{}},"source":["from sklearn.svm import LinearSVR\n","\n","svm_reg = LinearSVR(epsilon=1.5, random_state=42)\n","svm_reg.fit(X, y)\n","\n","svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1, gamma=\"auto\")\n","svm_poly_reg.fit(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zm9toFb8G0km","colab_type":"text"},"source":["# Decision Tree"]},{"cell_type":"code","metadata":{"id":"JZYUfWgqG0kn","colab_type":"code","colab":{}},"source":["from sklearn.tree import DecisionTreeClassifier\n","\n","tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n","tree_clf.fit(X, y)\n","tree_clf.predict([[5, 1.5]])\n","tree_clf.predict_proba([[5, 1.5]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s6dv_O5fG0kq","colab_type":"code","colab":{}},"source":["from sklearn.tree import DecisionTreeRegressor\n","\n","tree_reg = DecisionTreeRegressor(max_depth=2) \n","tree_reg.fit(X, y)\n","\n","# min_samples_split, min_samples_leaf, min_weight_fraction_leaf, and max_leaf_nodes\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cvhMlu_MG0ks","colab_type":"text"},"source":["# Ensemble Learning"]},{"cell_type":"code","metadata":{"id":"juWc58boG0kt","colab_type":"code","colab":{}},"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","\n","# Voting Classifier\n","log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n","rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n","svm_clf = SVC(gamma=\"auto\", random_state=42)\n","\n","voting_clf = VotingClassifier(\n","    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n","    voting='hard') # soft\n","voting_clf.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9a1V-khfG0kv","colab_type":"code","colab":{}},"source":["log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n","rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n","svm_clf = SVC(gamma=\"auto\", probability=True, random_state=42)\n","\n","voting_clf = VotingClassifier(\n","    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n","    voting='soft')\n","voting_clf.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PR91I1OPG0ky","colab_type":"text"},"source":["## Bagging and Pasting"]},{"cell_type":"code","metadata":{"id":"-IVjuSrGG0ky","colab_type":"code","colab":{},"outputId":"2baab294-5f49-48f2-84bd-9baa7b63a39e"},"source":["# Bagging ensembles\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","bag_clf = BaggingClassifier(\n","    DecisionTreeClassifier(random_state=42), n_estimators=500,\n","    max_samples=100, bootstrap=True, n_jobs=-1, oob_score = True, random_state=42) # boostrap = False for Pasting\n","bag_clf.fit(X_train, y_train)\n","\n","bag_clf.obb_score_\n","y_pred = bag_clf.predict(X_test)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'X_train' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-4adec7dfdbbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     max_samples=100, bootstrap=True, n_jobs=-1, random_state=42)\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mbag_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbag_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"MzSJCbpwG0k0","colab_type":"text"},"source":["## Random Forest"]},{"cell_type":"code","metadata":{"id":"KBoThT9AG0k1","colab_type":"code","colab":{}},"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)\n","rnd_clf.fit(X_train, y_train)\n","\n","y_pred_rf = rnd_clf.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wne3fqrDG0k2","colab_type":"text"},"source":["# Neural Networks"]},{"cell_type":"code","metadata":{"id":"-zz8OCNAG0k3","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))\n","\n","def relu(z):\n","    return np.maximum(0, z)\n","\n","def derivative(f, z, eps=0.000001):\n","    return (f(z + eps) - f(z - eps))/(2 * eps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gPPTFgzzG0k5","colab_type":"code","colab":{}},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QLbNYVqEG0k7","colab_type":"code","colab":{}},"source":["def leakrelu(z):\n","    return np.maximun(0.001z, z)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t1JDsxzjG0k8","colab_type":"text"},"source":["# TensorFlow"]},{"cell_type":"code","metadata":{"id":"oEc2VvYcG0k8","colab_type":"code","colab":{}},"source":["!pip install tensorflow-gpu==2.0.0-beta1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5XQJrNx_G0k-","colab_type":"code","colab":{}},"source":["!pip install -q tensorflow==2.0.0-alpha0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uBJY-dv1G0lA","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import tensorflow_datasets as tfds\n","\n","# tfds works in both Eager and Graph modes\n","tf.enable_eager_execution()\n","\n","# See available datasets\n","print(tfds.list_builders())\n","\n","# Construct a tf.data.Dataset\n","dataset = tfds.load(name=\"mnist\", split=tfds.Split.TRAIN)\n","\n","# Build your input pipeline\n","dataset = dataset.shuffle(1024).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n","for features in dataset.take(1):\n","    image, label = features[\"image\"], features[\"label\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gRqu89rjG0lC","colab_type":"code","colab":{}},"source":["tf.cast()\n","map()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CW1jsGUsG0lD","colab_type":"code","colab":{}},"source":["import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WaokBYpjG0lF","colab_type":"code","colab":{}},"source":["with tf.name_scope(\"dnn\"):\n","    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n","    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"hidden2\")\n","    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n","    \n","with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")\n","    \n","learning_rate = 0.01\n","\n","with tf.name_scope(\"train\"):\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","    training_op = optimizer.minimize(loss)\n","    \n","with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n","\n","with tf.Session() as sess:\n","    init.run()\n","    for epoch in range(n_epochs):\n","        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","        if epoch % 5 == 0:\n","            acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n","            acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n","\n","    save_path = saver.save(sess, \"./my_model_final.ckpt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"McpLqwwMG0lH","colab_type":"code","colab":{}},"source":["# Importing data\n","def train_input_fn(features, labels, batch_size):\n","    \"\"\"An input function for training\"\"\"\n","    # Convert the inputs to a Dataset.\n","    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n","\n","    # Shuffle, repeat, and batch the examples.\n","    return dataset.shuffle(1000).repeat().batch(batch_size)\n","\n","# Define three numeric feature columns.\n","population = tf.feature_column.numeric_column('population')\n","crime_rate = tf.feature_column.numeric_column('crime_rate')\n","median_education = tf.feature_column.numeric_column('median_education',\n","                    normalizer_fn=lambda x: x - global_education_mean)\n","\n","# Instantiate an estimator, passing the feature columns.\n","estimator = tf.estimator.LinearClassifier(\n","    feature_columns=[population, crime_rate, median_education])\n","\n","# `input_fn` is the function created in Step 1\n","estimator.train(input_fn=my_training_set, steps=2000)\n","estimator.evaluate()\n","estimator.predict()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U2aRnicFG0lJ","colab_type":"code","colab":{},"outputId":"60c630e2-8f1f-4b00-d84d-d38ea86ef617"},"source":["# Build a DNN with 2 hidden layers and 10 nodes in each hidden layer.\n","classifier = tf.estimator.DNNClassifier(\n","    feature_columns=my_feature_columns,\n","    # Two hidden layers of 10 nodes each.\n","    hidden_units=[10, 10],\n","    # The model must choose between 3 classes.\n","    n_classes=3)\n","\n","# Train the Model.\n","classifier.train(\n","    input_fn=lambda:iris_data.train_input_fn(train_x, train_y, args.batch_size),\n","    steps=args.train_steps)\n","\n","# Evaluate the model.\n","eval_result = classifier.evaluate(\n","    input_fn=lambda:iris_data.eval_input_fn(test_x, test_y, args.batch_size))\n","\n","print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n","\n","# Generate predictions from the model\n","expected = ['Setosa', 'Versicolor', 'Virginica']\n","predict_x = {\n","    'SepalLength': [5.1, 5.9, 6.9],\n","    'SepalWidth': [3.3, 3.0, 3.1],\n","    'PetalLength': [1.7, 4.2, 5.4],\n","    'PetalWidth': [0.5, 1.5, 2.1],\n","}\n","\n","predictions = classifier.predict(\n","    input_fn=lambda:iris_data.eval_input_fn(predict_x,\n","                                            batch_size=args.batch_size))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["type"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"xGeb5t3UG0lL","colab_type":"text"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"o3m69593G0lL","colab_type":"text"},"source":["A collection of datasets ready to use with TensorFlow.  \n","https://www.tensorflow.org/datasets"]},{"cell_type":"code","metadata":{"id":"fShZyICrG0lL","colab_type":"code","colab":{},"outputId":"0c702a08-ed5f-4944-9f70-e4845b0c6698"},"source":["!pip install tensorflow-datasets\n","# online environment"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting tensorflow-datasets\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/11/ce86b45cb41de8490d2f27f76ad5fe2f75d6eaac1c21f4a12baf1875882e/tensorflow_datasets-1.0.2-py3-none-any.whl (682kB)\n","\u001b[K    100% |████████████████████████████████| 686kB 2.1MB/s \n","\u001b[?25hCollecting promise (from tensorflow-datasets)\n","  Downloading https://files.pythonhosted.org/packages/5a/81/221d09d90176fd90aed4b530e31b8fedf207385767c06d1d46c550c5e418/promise-2.2.1.tar.gz\n","Requirement already satisfied: psutil in /Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages (from tensorflow-datasets) (5.6.1)\n","Requirement already satisfied: six in /Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages (from tensorflow-datasets) (1.12.0)\n","Requirement already satisfied: wrapt in /Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages (from tensorflow-datasets) (1.11.1)\n","Requirement already satisfied: numpy in /Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages (from tensorflow-datasets) (1.16.2)\n","Collecting tensorflow-metadata (from tensorflow-datasets)\n","  Downloading https://files.pythonhosted.org/packages/08/b7/3fc74574aa9aff44491cce996711dd6094653c20d9e2800be4efb054e0da/tensorflow_metadata-0.13.0-py3-none-any.whl\n","Requirement already satisfied: termcolor in /Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages (from tensorflow-datasets) (1.1.0)\n","Requirement already satisfied: tqdm in /Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages (from tensorflow-datasets) (4.31.1)\n","Requirement already satisfied: requests in /Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages (from tensorflow-datasets) (2.21.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages (from tensorflow-datasets) (3.7.0)\n","Collecting future (from tensorflow-datasets)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/52/e20466b85000a181e1e144fd8305caf2cf475e2f9674e797b222f8105f5f/future-0.17.1.tar.gz (829kB)\n","\u001b[K    100% |████████████████████████████████| 829kB 7.5MB/s \n","\u001b[?25hRequirement already satisfied: absl-py in /Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages (from tensorflow-datasets) (0.7.1)\n","Collecting dill (from tensorflow-datasets)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/7a/70803635c850e351257029089d38748516a280864c97cbc73087afef6d51/dill-0.3.0.tar.gz (151kB)\n","\u001b[K    100% |████████████████████████████████| 153kB 8.7MB/s \n","\u001b[?25hCollecting googleapis-common-protos (from tensorflow-metadata->tensorflow-datasets)\n","  Downloading https://files.pythonhosted.org/packages/eb/ee/e59e74ecac678a14d6abefb9054f0bbcb318a6452a30df3776f133886d7d/googleapis-common-protos-1.6.0.tar.gz\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages (from requests->tensorflow-datasets) (1.24.1)\n","Requirement already satisfied: idna<2.9,>=2.5 in /Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages (from requests->tensorflow-datasets) (2.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages (from requests->tensorflow-datasets) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages (from requests->tensorflow-datasets) (2019.6.16)\n","Requirement already satisfied: setuptools in /Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow-datasets) (40.8.0)\n","Building wheels for collected packages: promise, future, dill, googleapis-common-protos\n","  Building wheel for promise (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Stored in directory: /Users/zhutianwei/Library/Caches/pip/wheels/92/84/9f/75e2235effae0e1c5a5c0626a503e532bbffcb7e79e672b606\n","  Building wheel for future (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Stored in directory: /Users/zhutianwei/Library/Caches/pip/wheels/0c/61/d2/d6b7317325828fbb39ee6ad559dbe4664d0896da4721bf379e\n","  Building wheel for dill (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Stored in directory: /Users/zhutianwei/Library/Caches/pip/wheels/c9/de/a4/a91eec4eea652104d8c81b633f32ead5eb57d1b294eab24167\n","  Building wheel for googleapis-common-protos (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Stored in directory: /Users/zhutianwei/Library/Caches/pip/wheels/9e/3d/a2/1bec8bb7db80ab3216dbc33092bb7ccd0debfb8ba42b5668d5\n","Successfully built promise future dill googleapis-common-protos\n","Installing collected packages: promise, googleapis-common-protos, tensorflow-metadata, future, dill, tensorflow-datasets\n","Successfully installed dill-0.3.0 future-0.17.1 googleapis-common-protos-1.6.0 promise-2.2.1 tensorflow-datasets-1.0.2 tensorflow-metadata-0.13.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0KZqhgSFG0lP","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import tensorflow_datasets as tfds\n","\n","# tfds works in both Eager and Graph modes\n","tf.enable_eager_execution()\n","\n","# See available datasets\n","print(tfds.list_builders())\n","\n","# Construct a tf.data.Dataset\n","dataset = tfds.load(name=\"mnist\", split=tfds.Split.TRAIN)\n","\n","# Build your input pipeline\n","dataset = dataset.shuffle(1024).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n","for features in dataset.take(1):\n","    image, label = features[\"image\"], features[\"label\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"90LyDx-0G0lR","colab_type":"text"},"source":["## TensorFlow server"]},{"cell_type":"code","metadata":{"id":"nDidhhcyG0lR","colab_type":"code","colab":{}},"source":["tf.train.Server"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8iOsPgeTG0lS","colab_type":"text"},"source":["# Keras"]},{"cell_type":"code","metadata":{"id":"0fH855qRG0lT","colab_type":"code","colab":{},"outputId":"2b76297e-0da1-4f80-cf36-f846b24010c3"},"source":["import tensorflow as tf"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<module 'tensorflow.data' from '/Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages/tensorflow/data/__init__.py'>"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"jh7-UJU5G0lU","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","\n","model = Sequential([Dense(32, input_shape=(16,))])\n","# now the model will take as input arrays of shape (*, 16)\n","# and output arrays of shape (*, 32)\n","model.add(Dense(64, activation='relu', input_dim=100))\n","model.add(Dense(10, activation='softmax'))\n","\n","model.compile(optimizer='sgd', \n","              loss='mean_squared_error',\n","              metrics=['accuracy'])\n","model.fit(xs, ys, epochs=500)\n","model.evaluate()\n","model.predict([10.0]) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jOYzgX6YG0lW","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten, Dropout\n","mnist = tf.keras.datasets.mnist\n","\n","(x_train, y_train),(x_test, y_test) = mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","model = Sequential([\n","  Flatten(input_shape=(28, 28)),\n","  Dense(512, activation=tf.nn.relu),\n","  Dropout(0.2),\n","  Dense(10, activation=tf.nn.softmax)\n","])\n","\n","x_train\n","\n","# model.compile(optimizer='adam',\n","#               loss='sparse_categorical_crossentropy',\n","#               metrics=['accuracy'])\n","\n","# model.fit(x_train, y_train, epochs=5) # batch_size = 32`\n","# model.evaluate(x_test, y_test) # batch_size = 128\n","# model.predict(x_test) # batch_size = 128"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GUI4wEOFG0lY","colab_type":"text"},"source":["## Keras backends\n","https://keras.io/backend/  \n","the TensorFlow backend"]},{"cell_type":"code","metadata":{"id":"3LubFH7uG0lZ","colab_type":"code","colab":{},"outputId":"3d72872b-d3e8-4548-ae33-83d87c224ae1"},"source":["from keras import backend as K"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]}]}