{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Intro-to-Machine-Learning\" data-toc-modified-id=\"Intro-to-Machine-Learning-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Intro to Machine Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-the-data\" data-toc-modified-id=\"Get-the-data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Get the data</a></span></li><li><span><a href=\"#Visualization.-Data-Science\" data-toc-modified-id=\"Visualization.-Data-Science-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Visualization. Data Science</a></span></li><li><span><a href=\"#Prepare-the-data-for-Machine-Learning-algorithms\" data-toc-modified-id=\"Prepare-the-data-for-Machine-Learning-algorithms-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Prepare the data for Machine Learning algorithms</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Cleaning\" data-toc-modified-id=\"Data-Cleaning-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Data Cleaning</a></span></li><li><span><a href=\"#Handling-Text-and-Categorical-Attributes\" data-toc-modified-id=\"Handling-Text-and-Categorical-Attributes-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Handling Text and Categorical Attributes</a></span></li><li><span><a href=\"#Custom-Transformers\" data-toc-modified-id=\"Custom-Transformers-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Custom Transformers</a></span></li><li><span><a href=\"#Feature-Scaling\" data-toc-modified-id=\"Feature-Scaling-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>Feature Scaling</a></span></li><li><span><a href=\"#Transformation-Pipelines\" data-toc-modified-id=\"Transformation-Pipelines-1.3.5\"><span class=\"toc-item-num\">1.3.5&nbsp;&nbsp;</span>Transformation Pipelines</a></span></li></ul></li><li><span><a href=\"#Select-and-train-a-model\" data-toc-modified-id=\"Select-and-train-a-model-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Select and train a model</a></span></li><li><span><a href=\"#Fine-tune-your-model\" data-toc-modified-id=\"Fine-tune-your-model-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Fine-tune your model</a></span></li></ul></li><li><span><a href=\"#Classification\" data-toc-modified-id=\"Classification-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Performance-Measures\" data-toc-modified-id=\"Performance-Measures-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Performance Measures</a></span><ul class=\"toc-item\"><li><span><a href=\"#Error-Analysis\" data-toc-modified-id=\"Error-Analysis-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Error Analysis</a></span></li></ul></li></ul></li><li><span><a href=\"#Training-Models\" data-toc-modified-id=\"Training-Models-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Training Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Linear-Regression\" data-toc-modified-id=\"Linear-Regression-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Linear Regression</a></span></li><li><span><a href=\"#Polynomial-Regression\" data-toc-modified-id=\"Polynomial-Regression-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Polynomial Regression</a></span></li><li><span><a href=\"#Learning-Curves\" data-toc-modified-id=\"Learning-Curves-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Learning Curves</a></span></li><li><span><a href=\"#Regularized-Linear-Models\" data-toc-modified-id=\"Regularized-Linear-Models-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Regularized Linear Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Ridge-Regression\" data-toc-modified-id=\"Ridge-Regression-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Ridge Regression</a></span></li><li><span><a href=\"#Lasso-Regression\" data-toc-modified-id=\"Lasso-Regression-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>Lasso Regression</a></span></li><li><span><a href=\"#Elastic-Net\" data-toc-modified-id=\"Elastic-Net-3.4.3\"><span class=\"toc-item-num\">3.4.3&nbsp;&nbsp;</span>Elastic Net</a></span></li><li><span><a href=\"#Early-Stopping\" data-toc-modified-id=\"Early-Stopping-3.4.4\"><span class=\"toc-item-num\">3.4.4&nbsp;&nbsp;</span>Early Stopping</a></span></li></ul></li><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Logistic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Softmax-Regression\" data-toc-modified-id=\"Softmax-Regression-3.5.1\"><span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>Softmax Regression</a></span></li></ul></li></ul></li><li><span><a href=\"#SVM\" data-toc-modified-id=\"SVM-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>SVM</a></span><ul class=\"toc-item\"><li><span><a href=\"#Linear-SVM-Classification\" data-toc-modified-id=\"Linear-SVM-Classification-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Linear SVM Classification</a></span></li><li><span><a href=\"#Nonlinear-SVM-Classification\" data-toc-modified-id=\"Nonlinear-SVM-Classification-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Nonlinear SVM Classification</a></span></li><li><span><a href=\"#SVM-Regression\" data-toc-modified-id=\"SVM-Regression-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>SVM Regression</a></span></li></ul></li><li><span><a href=\"#Decision-Tree\" data-toc-modified-id=\"Decision-Tree-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Decision Tree</a></span></li><li><span><a href=\"#Ensemble-Learning\" data-toc-modified-id=\"Ensemble-Learning-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Ensemble Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bagging-and-Pasting\" data-toc-modified-id=\"Bagging-and-Pasting-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Bagging and Pasting</a></span></li><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Random Forest</a></span></li></ul></li><li><span><a href=\"#Neural-Networks\" data-toc-modified-id=\"Neural-Networks-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Neural Networks</a></span></li><li><span><a href=\"#TensorFlow\" data-toc-modified-id=\"TensorFlow-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>TensorFlow</a></span></li><li><span><a href=\"#Keras\" data-toc-modified-id=\"Keras-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Keras</a></span></li></ul></div>"
      ],
      "metadata": {
        "toc": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%conda install seaborn"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to Machine Learning"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization. Data Science"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the data for Machine Learning algorithms"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Cleaning"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "imputer.fit(housing_num) # Calculate the median of each column\n",
        "X = imputer.transform(housing_num) \n",
        "\n",
        "# X = imputer.fit_transform(housing_num)"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Text and Categorical Attributes"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "cat_encoder = OneHotEncoder()\n",
        "housing_cat_1hot = cat_encoder.fit_transform(housing_cat) # sparse matrix\n",
        "housing_cat_1hot.toarray() # array "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Transformers"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "attr_adder = FunctionTransformer(add_extra_features, validate=False,\n",
        "                                 kw_args={\"add_bedrooms_per_room\": False})\n",
        "housing_extra_attribs = attr_adder.fit_transform(housing.values)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Scaling"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformation Pipelines"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "num_pipeline = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
        "        ('attribs_adder', FunctionTransformer(add_extra_features, validate=False)),\n",
        "        ('std_scaler', StandardScaler()),\n",
        "    ])\n",
        "\n",
        "housing_num_tr = num_pipeline.fit_transform(housing_num)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# earlier versions of the book applied different transformations to different columns using a solution \n",
        "# based on a DataFrameSelector transformer and a FeatureUnion (see below). \n",
        "# It is now preferable to use the ColumnTransformer class\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "num_attribs = list(housing_num)\n",
        "cat_attribs = [\"ocean_proximity\"]\n",
        "\n",
        "full_pipeline = ColumnTransformer([\n",
        "        (\"num\", num_pipeline, num_attribs),\n",
        "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
        "    ])\n",
        "\n",
        "housing_prepared = full_pipeline.fit_transform(housing)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import FeatureUnion\n",
        "\n",
        "num_attribs = list(housing_num)\n",
        "cat_attribs = [\"ocean_proximity\"]\n",
        "\n",
        "old_num_pipeline = Pipeline([\n",
        "        ('selector', OldDataFrameSelector(num_attribs)),\n",
        "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
        "        ('attribs_adder', FunctionTransformer(add_extra_features, validate=False)),\n",
        "        ('std_scaler', StandardScaler()),\n",
        "    ])\n",
        "\n",
        "old_cat_pipeline = Pipeline([\n",
        "        ('selector', OldDataFrameSelector(cat_attribs)),\n",
        "        ('cat_encoder', OneHotEncoder(sparse=False)),\n",
        "    ])\n",
        "\n",
        "old_full_pipeline = FeatureUnion(transformer_list=[\n",
        "        (\"num_pipeline\", old_num_pipeline),\n",
        "        (\"cat_pipeline\", old_cat_pipeline),\n",
        "    ])\n",
        "\n",
        "old_housing_prepared = old_full_pipeline.fit_transform(housing) # The result is the same as with the ColumnTransformer"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select and train a model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.learning_curve import validation_curve\n",
        "degree = np.arange(0, 21) \n",
        "train_score, val_score = validation_curve(PolynomialRegression(), X, y, 'polynomialfeatures__degree', degree, cv=7)\n",
        "plt.plot(degree, np.median(train_score, 1), color='blue', label='training score')\n",
        "plt.plot(degree, np.median(val_score, 1), color='red', label='validation score')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.learning_curve import learning_curve"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(housing_prepared, housing_labels)\n",
        "lin_reg.predict(some_data_prepared)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg = DecisionTreeRegressor(random_state=42)\n",
        "tree_reg.fit(housing_prepared, housing_labels)\n",
        "housing_predictions = tree_reg.predict(housing_prepared)\n",
        "\n",
        "forest_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "forest_rmse = np.sqrt(forest_mse)\n",
        "\n",
        "accuracy_score(housing_labels, housing_predictions)\n",
        "x = confusion_matrix(housing_labels, housing_predictions)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
        "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
        "tree_rmse_scores = np.sqrt(-scores)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "forest_reg = RandomForestRegressor(n_estimators=10, random_state=42)\n",
        "forest_reg.fit(housing_prepared, housing_labels)\n",
        "housing_predictions = forest_reg.predict(housing_prepared)\n",
        "\n",
        "forest_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "forest_rmse = np.sqrt(forest_mse)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "svm_reg = SVR(kernel=\"linear\")\n",
        "svm_reg.fit(housing_prepared, housing_labels)\n",
        "housing_predictions = svm_reg.predict(housing_prepared)\n",
        "\n",
        "svm_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "svm_rmse = np.sqrt(svm_mse)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune your model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is an example of using grid search to find the optimal polynomial model.\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {'polynomialfeatures__degree': np.arange(21),\n",
        "              'linearregression__fit_intercept': [True, False], \n",
        "              'linearregression__normalize': [True, False]}\n",
        "\n",
        "grid = GridSearchCV(PolynomialRegression(), param_grid, cv=7)\n",
        "grid.fit(X, y)\n",
        "grid.best_params_\n",
        "model = grid.best_estimator_"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# The following code searches for the best combination of hyperparameter values for the RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = [\n",
        "    # try 12 (3×4) combinations of hyperparameters\n",
        "    {'n_estimators': [3, 10, 30], \n",
        "     'max_features': [2, 4, 6, 8]},\n",
        "    # then try 6 (2×3) combinations with bootstrap set as False\n",
        "    {'bootstrap': [False], \n",
        "     'n_estimators': [3, 10], \n",
        "     'max_features': [2, 3, 4]},\n",
        "  ]\n",
        "\n",
        "forest_reg = RandomForestRegressor(random_state=42)\n",
        "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
        "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
        "                           scoring='neg_mean_squared_error', return_train_score=True)\n",
        "grid_search.fit(housing_prepared, housing_labels)\n",
        "grid_search.best_params_\n",
        "model = grid_search.best_estimator_"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "param_distribs = {\n",
        "        'n_estimators': randint(low=1, high=200),\n",
        "        'max_features': randint(low=1, high=8),\n",
        "    }\n",
        "\n",
        "forest_reg = RandomForestRegressor(random_state=42)\n",
        "rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n",
        "                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
        "rnd_search.fit(housing_prepared, housing_labels)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Anaylize the Best Models and Their Errors¶\n",
        "feature_importances = grid_search.best_estimator_.feature_importances_\n",
        "\n",
        "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
        "#cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"] # old solution\n",
        "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
        "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
        "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
        "sorted(zip(feature_importances, attributes), reverse=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on the Test Set\n",
        "final_model = grid_search.best_estimator_\n",
        "print(type(final_model))\n",
        "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
        "y_test = strat_test_set[\"median_house_value\"].copy()\n",
        "\n",
        "X_test_prepared = full_pipeline.transform(X_test)\n",
        "final_predictions = final_model.predict(X_test_prepared)\n",
        "\n",
        "final_mse = mean_squared_error(y_test, final_predictions)\n",
        "final_rmse = np.sqrt(final_mse)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Measures"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv = 3)\n",
        "y_train_pred"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "precision_score(y_train_5, y_train_pred)\n",
        "recall_score(y_train_5, y_train_pred)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1_score(y_train_5, y_train_pred)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n",
        "                             method=\"decision_function\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Error Analysis"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Models"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X, y)\n",
        "lin_reg.predict(X_new)\n",
        "lin_reg.intercept_, lin_reg.coef_"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Polynomial Regression"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_poly, y)\n",
        "lin_reg.predict(X_poly_new)\n",
        "lin_reg.intercept_, lin_reg.coef_"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Curves"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def plot_learning_curves(model, X, y):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)\n",
        "    train_errors, val_errors = [], []\n",
        "    \n",
        "    for m in range(1, len(X_train)):\n",
        "        model.fit(X_train[:m], y_train[:m])\n",
        "        y_train_predict = model.predict(X_train[:m])\n",
        "        y_val_predict = model.predict(X_val)\n",
        "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
        "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
        "\n",
        "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
        "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
        "    plt.legend(loc=\"upper right\", fontsize=14)   # not shown in the book\n",
        "    plt.xlabel(\"Training set size\", fontsize=14) # not shown\n",
        "    plt.ylabel(\"RMSE\", fontsize=14)              # not shown"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "lin_reg = LinearRegression()\n",
        "plot_learning_curves(lin_reg, X, y)\n",
        "plt.axis([0, 80, 0, 3])                         # not shown in the book\n",
        "save_fig(\"underfitting_learning_curves_plot\")   # not shown\n",
        "plt.show()                                      # not shown"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "polynomial_regression = Pipeline([\n",
        "        (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
        "        (\"lin_reg\", LinearRegression()),\n",
        "    ])\n",
        "\n",
        "plot_learning_curves(polynomial_regression, X, y)\n",
        "plt.axis([0, 80, 0, 3])           # not shown\n",
        "save_fig(\"learning_curves_plot\")  # not shown\n",
        "plt.show()                        # not shown"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regularized Linear Models"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ridge Regression"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "ridge_reg = Ridge(alpha=1, solver=\"cholesky\", random_state=42)\n",
        "ridge_reg.fit(X, y)\n",
        "ridge_reg.predict([[1.5]])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Stochastic Gradient Descent\n",
        "sgd_reg = SGDRegressor(max_iter=50, tol=-np.infty, penalty=\"l2\", random_state=42)\n",
        "sgd_reg.fit(X, y.ravel())\n",
        "sgd_reg.predict([[1.5]])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ridge_reg = Ridge(alpha=1, solver=\"sag\", random_state=42)\n",
        "ridge_reg.fit(X, y)\n",
        "ridge_reg.predict([[1.5]])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lasso Regression"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "lasso_reg = Lasso(alpha=0.1)\n",
        "lasso_reg.fit(X, y)\n",
        "lasso_reg.predict([[1.5]])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Elastic Net"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
        "elastic_net.fit(X, y)\n",
        "elastic_net.predict([[1.5]])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Early Stopping"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import clone\n",
        "sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True, penalty=None,\n",
        "                       learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
        "\n",
        "minimum_val_error = float(\"inf\")\n",
        "best_epoch = None\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1000):\n",
        "    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off\n",
        "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
        "    val_error = mean_squared_error(y_val, y_val_predict)\n",
        "    \n",
        "    if val_error < minimum_val_error:\n",
        "        minimum_val_error = val_error\n",
        "        best_epoch = epoch\n",
        "        best_model = clone(sgd_reg)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "best_epoch, best_model"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax Regression"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear SVM Classification "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Not recommended\n",
        "svm_clf = SVC(kernel=\"linear\", C=float(\"inf\"))\n",
        "svm_clf.fit(X, y)\n",
        "\n",
        "# SVM Classifier model\n",
        "svm_clf = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\", random_state=42)),\n",
        "    ])\n",
        "\n",
        "svm_clf.fit(X, y)\n",
        "svm_clf.predict([[5.5, 1.7]])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nonlinear SVM Classification"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "\n",
        "\n",
        "polynomial_svm_clf = Pipeline([\n",
        "        (\"poly_features\", PolynomialFeatures(degree=3)),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\", random_state=42))\n",
        "    ])\n",
        "\n",
        "polynomial_svm_clf.fit(X, y)\n",
        "\n",
        "# Polynomial Kernel\n",
        "poly_kernel_svm_clf = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
        "    ])\n",
        "poly_kernel_svm_clf.fit(X, y)\n",
        "\n",
        "# Gaussian RBF Kernel\n",
        "rbf_kernel_svm_clf = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
        "    ])\n",
        "rbf_kernel_svm_clf.fit(X, y)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM Regression"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVR\n",
        "\n",
        "svm_reg = LinearSVR(epsilon=1.5, random_state=42)\n",
        "svm_reg.fit(X, y)\n",
        "\n",
        "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1, gamma=\"auto\")\n",
        "svm_poly_reg.fit(X, y)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
        "tree_clf.fit(X, y)\n",
        "tree_clf.predict([[5, 1.5]])\n",
        "tree_clf.predict_proba([[5, 1.5]])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg = DecisionTreeRegressor(max_depth=2) \n",
        "tree_reg.fit(X, y)\n",
        "\n",
        "# min_samples_split, min_samples_leaf, min_weight_fraction_leaf, and max_leaf_nodes\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Learning"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Voting Classifier\n",
        "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
        "rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
        "svm_clf = SVC(gamma=\"auto\", random_state=42)\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting='hard') # soft\n",
        "voting_clf.fit(X_train, y_train)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
        "rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
        "svm_clf = SVC(gamma=\"auto\", probability=True, random_state=42)\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting='soft')\n",
        "voting_clf.fit(X_train, y_train)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bagging and Pasting"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Bagging ensembles\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
        "    max_samples=100, bootstrap=True, n_jobs=-1, oob_score = True, random_state=42) # boostrap = False for Pasting\n",
        "bag_clf.fit(X_train, y_train)\n",
        "\n",
        "bag_clf.obb_score_\n",
        "y_pred = bag_clf.predict(X_test)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-4adec7dfdbbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     max_samples=100, bootstrap=True, n_jobs=-1, random_state=42)\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mbag_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbag_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ],
      "execution_count": 13,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)\n",
        "rnd_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rnd_clf.predict(X_test)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def derivative(f, z, eps=0.000001):\n",
        "    return (f(z + eps) - f(z - eps))/(2 * eps)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def leakrelu(z):\n",
        "    return np.maximun(0.001z, z)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TensorFlow"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-gpu==2.0.0-beta1"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow==2.0.0-alpha0"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# tfds works in both Eager and Graph modes\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "# See available datasets\n",
        "print(tfds.list_builders())\n",
        "\n",
        "# Construct a tf.data.Dataset\n",
        "dataset = tfds.load(name=\"mnist\", split=tfds.Split.TRAIN)\n",
        "\n",
        "# Build your input pipeline\n",
        "dataset = dataset.shuffle(1024).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "for features in dataset.take(1):\n",
        "    image, label = features[\"image\"], features[\"label\"]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "tf.cast()\n",
        "map()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"hidden2\")\n",
        "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
        "    \n",
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "    \n",
        "learning_rate = 0.01\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(loss)\n",
        "    \n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        if epoch % 5 == 0:\n",
        "            acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "            acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing data\n",
        "def train_input_fn(features, labels, batch_size):\n",
        "    \"\"\"An input function for training\"\"\"\n",
        "    # Convert the inputs to a Dataset.\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
        "\n",
        "    # Shuffle, repeat, and batch the examples.\n",
        "    return dataset.shuffle(1000).repeat().batch(batch_size)\n",
        "\n",
        "# Define three numeric feature columns.\n",
        "population = tf.feature_column.numeric_column('population')\n",
        "crime_rate = tf.feature_column.numeric_column('crime_rate')\n",
        "median_education = tf.feature_column.numeric_column('median_education',\n",
        "                    normalizer_fn=lambda x: x - global_education_mean)\n",
        "\n",
        "# Instantiate an estimator, passing the feature columns.\n",
        "estimator = tf.estimator.LinearClassifier(\n",
        "    feature_columns=[population, crime_rate, median_education])\n",
        "\n",
        "# `input_fn` is the function created in Step 1\n",
        "estimator.train(input_fn=my_training_set, steps=2000)\n",
        "estimator.evaluate()\n",
        "estimator.predict()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a DNN with 2 hidden layers and 10 nodes in each hidden layer.\n",
        "classifier = tf.estimator.DNNClassifier(\n",
        "    feature_columns=my_feature_columns,\n",
        "    # Two hidden layers of 10 nodes each.\n",
        "    hidden_units=[10, 10],\n",
        "    # The model must choose between 3 classes.\n",
        "    n_classes=3)\n",
        "\n",
        "# Train the Model.\n",
        "classifier.train(\n",
        "    input_fn=lambda:iris_data.train_input_fn(train_x, train_y, args.batch_size),\n",
        "    steps=args.train_steps)\n",
        "\n",
        "# Evaluate the model.\n",
        "eval_result = classifier.evaluate(\n",
        "    input_fn=lambda:iris_data.eval_input_fn(test_x, test_y, args.batch_size))\n",
        "\n",
        "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n",
        "\n",
        "# Generate predictions from the model\n",
        "expected = ['Setosa', 'Versicolor', 'Virginica']\n",
        "predict_x = {\n",
        "    'SepalLength': [5.1, 5.9, 6.9],\n",
        "    'SepalWidth': [3.3, 3.0, 3.1],\n",
        "    'PetalLength': [1.7, 4.2, 5.4],\n",
        "    'PetalWidth': [0.5, 1.5, 2.1],\n",
        "}\n",
        "\n",
        "predictions = classifier.predict(\n",
        "    input_fn=lambda:iris_data.eval_input_fn(predict_x,\n",
        "                                            batch_size=args.batch_size))"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": [
              "type"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "A collection of datasets ready to use with TensorFlow.  \n",
        "https://www.tensorflow.org/datasets"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-datasets\n",
        "# online environment"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/11/ce86b45cb41de8490d2f27f76ad5fe2f75d6eaac1c21f4a12baf1875882e/tensorflow_datasets-1.0.2-py3-none-any.whl (682kB)\n",
            "\u001b[K    100% |████████████████████████████████| 686kB 2.1MB/s \n",
            "\u001b[?25hCollecting promise (from tensorflow-datasets)\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/81/221d09d90176fd90aed4b530e31b8fedf207385767c06d1d46c550c5e418/promise-2.2.1.tar.gz\n",
            "Requirement already satisfied: psutil in /Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages (from tensorflow-datasets) (5.6.1)\n",
            "Requirement already satisfied: six in /Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages (from tensorflow-datasets) (1.12.0)\n",
            "Requirement already satisfied: wrapt in /Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages (from tensorflow-datasets) (1.11.1)\n",
            "Requirement already satisfied: numpy in /Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages (from tensorflow-datasets) (1.16.2)\n",
            "Collecting tensorflow-metadata (from tensorflow-datasets)\n",
            "  Downloading https://files.pythonhosted.org/packages/08/b7/3fc74574aa9aff44491cce996711dd6094653c20d9e2800be4efb054e0da/tensorflow_metadata-0.13.0-py3-none-any.whl\n",
            "Requirement already satisfied: termcolor in /Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages (from tensorflow-datasets) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages (from tensorflow-datasets) (4.31.1)\n",
            "Requirement already satisfied: requests in /Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages (from tensorflow-datasets) (2.21.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages (from tensorflow-datasets) (3.7.0)\n",
            "Collecting future (from tensorflow-datasets)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/52/e20466b85000a181e1e144fd8305caf2cf475e2f9674e797b222f8105f5f/future-0.17.1.tar.gz (829kB)\n",
            "\u001b[K    100% |████████████████████████████████| 829kB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py in /Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages (from tensorflow-datasets) (0.7.1)\n",
            "Collecting dill (from tensorflow-datasets)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/7a/70803635c850e351257029089d38748516a280864c97cbc73087afef6d51/dill-0.3.0.tar.gz (151kB)\n",
            "\u001b[K    100% |████████████████████████████████| 153kB 8.7MB/s \n",
            "\u001b[?25hCollecting googleapis-common-protos (from tensorflow-metadata->tensorflow-datasets)\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/ee/e59e74ecac678a14d6abefb9054f0bbcb318a6452a30df3776f133886d7d/googleapis-common-protos-1.6.0.tar.gz\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages (from requests->tensorflow-datasets) (1.24.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages (from requests->tensorflow-datasets) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages (from requests->tensorflow-datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages (from requests->tensorflow-datasets) (2019.6.16)\n",
            "Requirement already satisfied: setuptools in /Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow-datasets) (40.8.0)\n",
            "Building wheels for collected packages: promise, future, dill, googleapis-common-protos\n",
            "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /Users/zhutianwei/Library/Caches/pip/wheels/92/84/9f/75e2235effae0e1c5a5c0626a503e532bbffcb7e79e672b606\n",
            "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /Users/zhutianwei/Library/Caches/pip/wheels/0c/61/d2/d6b7317325828fbb39ee6ad559dbe4664d0896da4721bf379e\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /Users/zhutianwei/Library/Caches/pip/wheels/c9/de/a4/a91eec4eea652104d8c81b633f32ead5eb57d1b294eab24167\n",
            "  Building wheel for googleapis-common-protos (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /Users/zhutianwei/Library/Caches/pip/wheels/9e/3d/a2/1bec8bb7db80ab3216dbc33092bb7ccd0debfb8ba42b5668d5\n",
            "Successfully built promise future dill googleapis-common-protos\n",
            "Installing collected packages: promise, googleapis-common-protos, tensorflow-metadata, future, dill, tensorflow-datasets\n",
            "Successfully installed dill-0.3.0 future-0.17.1 googleapis-common-protos-1.6.0 promise-2.2.1 tensorflow-datasets-1.0.2 tensorflow-metadata-0.13.0\n"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# tfds works in both Eager and Graph modes\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "# See available datasets\n",
        "print(tfds.list_builders())\n",
        "\n",
        "# Construct a tf.data.Dataset\n",
        "dataset = tfds.load(name=\"mnist\", split=tfds.Split.TRAIN)\n",
        "\n",
        "# Build your input pipeline\n",
        "dataset = dataset.shuffle(1024).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "for features in dataset.take(1):\n",
        "    image, label = features[\"image\"], features[\"label\"]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TensorFlow server"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "tf.train.Server"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keras"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": [
              "<module 'tensorflow.data' from '/Users/zhutianwei/anaconda/envs/IntroToTensorFlow/lib/python3.6/site-packages/tensorflow/data/__init__.py'>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential([Dense(32, input_shape=(16,))])\n",
        "# now the model will take as input arrays of shape (*, 16)\n",
        "# and output arrays of shape (*, 32)\n",
        "model.add(Dense(64, activation='relu', input_dim=100))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='sgd', \n",
        "              loss='mean_squared_error',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(xs, ys, epochs=500)\n",
        "model.evaluate()\n",
        "model.predict([10.0]) "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "model = Sequential([\n",
        "  Flatten(input_shape=(28, 28)),\n",
        "  Dense(512, activation=tf.nn.relu),\n",
        "  Dropout(0.2),\n",
        "  Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "x_train\n",
        "\n",
        "# model.compile(optimizer='adam',\n",
        "#               loss='sparse_categorical_crossentropy',\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "# model.fit(x_train, y_train, epochs=5) # batch_size = 32`\n",
        "# model.evaluate(x_test, y_test) # batch_size = 128\n",
        "# model.predict(x_test) # batch_size = 128"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keras backends\n",
        "https://keras.io/backend/  \n",
        "the TensorFlow backend"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ],
      "execution_count": 15,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "toc": {
      "toc_position": {},
      "skip_h1_title": false,
      "number_sections": true,
      "title_cell": "Table of Contents",
      "toc_window_display": false,
      "base_numbering": 1,
      "toc_section_display": true,
      "title_sidebar": "Contents",
      "toc_cell": true,
      "nav_menu": {
        "height": "299px",
        "width": "351px"
      },
      "sideBar": false
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "0.15.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}